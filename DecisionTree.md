# 决策树
## 决策树模型
决策树是一种基本的分类回归方法，决策树由结点与有向边组成。结点分为內部结点与叶结点。內部结点表示一个特征或属性，叶结点表示一个类。决策树学习包括三个步骤：特征选择、决策树生成与决策树剪枝。

决策树等价于一个 ```if-then``` 规则的集合。决策树的根节点到叶结点的每一条路径构建一条规则；路径上的內部结点对应着规则的条件，叶结点对应规则的结论。决策树的规则的性质：互斥且完备，即每个实例都被一条且仅有一条路径覆盖。

叶结点中大多数实例属于某个类，则将该叶结点划分为该类。

决策树学习本质上是从训练数据中归纳出一组分类规则，与训练集不相矛盾矛盾的树可能有多个或者没有，应选择与训练数据矛盾较小的树同时具有较好的泛化能力。决策树学习也可以看作是由训练数据集估计条件概率模型。决策树的损失函数通常为正则化的极大似然函数。

从所有可能的决策树中选取最优的决策树是 NP 完全问题，所以实际中一般采用启发式的学习算法，求解次优决策树。决策树的学习算法通常是一个递归的选择最优特征，并根据该特征对训练数据进行分割，使得对各个子数据集有一个最好的分类过程。

选取最优特征最为根结点，按照这一特征将训练集分割成子集，使得各子集有一个当前条件下的最好分类。如果这些子集能够被基本分类正确，则构建叶结点，并将子集分到对应叶结点中；如果还有子集不能被基本分类正确，则对这些子集选择新的最优特征，继续对其进行分割，构建相应的结点。如此递归的进行下去，直至所有训练数据子集被基本分类正确或没有合适特征为止。最后每一个子集都被分到叶结点上，就生成了一颗决策树。

为避免发生过拟合现象，还需对决策树进行自下而上的**剪枝**，决策树的生成对应模型的局部选择（局部最优），决策树的剪枝对应模型的全局选择（全局最优）。
## 特征选择
特征选择的准则是信息增益或信息增益比。对于训练集 $D$ 和特征$A$，记：

$\left | D \right |$为样本容量，设有$K$个类$C_{k}$ , $k=1,2,...,K$, $\left | C_{k} \right |$表示属于类$C_{k}$的样本个数。$\sum_{k=1}^{K}\left | C_{k} \right |=\left | D \right |$, 设特征$A$有$n$个不同取值，根据特征$A$的取值能将$D$划分为$n$个子集$D_{1},D_{2},...,D_{n}$ ，$\sum_{i=1}^{n}\left | D_{i} \right |=\left | D \right |$， 记子集$D_{i}$中属于类$C_{k}$的集合为$D_{ik}$，$\sum_{k=1}^{K}\left | D_{ik} \right |=\left | D_{i}\right |$。信息增益计算如下：


$$H\left ( D \right )=-\sum_{k=1}^{K}\frac{\left | C_{k} \right |}{\left | D \right |}log_{2}\frac{\left | C_{k} \right |}{\left | D \right |}$$
$$H\left ( D|A \right )=\sum_{i=1}^{n}\frac{\left | D_{i} \right |}{\left | D \right |}H\left ( D_{i} \right )=-\sum_{i=1}^{n}\frac{\left | D_{i} \right |}{\left | D \right |}\sum_{k=1}^{K}\frac{\left | D_{ik} \right |}{\left | D_{i} \right |}log_{2}\frac{\left | D_{ik} \right |}{\left | D_{i} \right |}$$
$$g\left ( D,A \right )=H\left ( D \right )-H\left ( D|A \right )$$
信息增益实际上为类 $D$ 和特征 $A$ 之间的互信息。

条件熵：
$$H\left ( Y|X \right )=\sum_{i=1}^{n}p_{i}H\left ( Y|X=x_{i} \right )$$
$$p_{i}=P\left ( X=x_{i} \right )$$

以信息熵作为划分训练数据集的特征，存在偏向于选择取值较多的特征的问题，使用信息增益比可对该问题进行矫正。信息增益比：
$$g_{R}\left ( D,A \right )=\frac{g\left ( D,A \right )}{H_{A}\left ( D \right )}$$
$$H_{A}\left ( D \right )=-\sum_{i=1}^{n}\frac{\left | D_{i} \right |}{\left | D \right |}log_{2}\frac{\left | D_{i} \right |}{\left | D \right |}=H\left ( A \right )$$
## 决策树的生成
**ID3算法**
>输入：训练数据集$D$，特征集$A$，阈值 $\varepsilon$；
> 
>输出：决策树 $T$
>
>1. 若 $D$ 中所有实列输入同一类 $C_{k}$，则 $T$ 为单结点树，并将类 $C_{k}$ 作为该结点的类标记，返回$T$;  
>2. 若 $A=\varnothing$，则 $T$ 为单结点树，并将 $D$ 中实列数量最大的类 $C_{k}$ 作为该结点的类标记，返回$T$;  
>3. 否则，计算 $A$ 中各特征对 $D$ 的信息增益，选择信息增益最大的特征 $A_{g}$；  
>4. 如果 $A_{g}$ 的信息增益小于阈值 $\varepsilon$，则置 $T$ 为单结点树，将 $D$ 中实列数量最大的类 $C_{k}$ 作为该结点的类标记，返回$T$;  
>5. 否则，对 $A_{g}$ 的每一可能取值 $a_{i}$，依 $A_{g}=a_{i}$ 将 $D$ 分割为若干非空子集 $D_{i}$，将 $D_{i}$ 中实例数量最大的类最为标记，构建子节点，由结点及其子结点构成树 $T$，返回 $T$;  
>6. 对第 $i$ 个子结点，以 $D_{i}$ 为训练集，以 $A-\{A_{g}\}$ 为特征集，递归调用 1—5，得到子树 $T_{i}$，返回 $T_{i}$；

**ID3算法只有树的生成，所以该算法生成的树容易产生过拟合**  
**C4.5算法与ID3算法相似，不过在特征选择上将信息增益用信息增益比替代**  

## 决策树的剪枝
决策树生成过程中过多考虑提高对训练集的分类准确性，从而构建出过于复杂的决策树。可以通过简化决策树（剪枝）来解决过拟合的问题。  
决策树剪枝通过极小化决策树的整体的损失函数来实现。设树 $T$ 的叶结点个数为 $\left | T \right |$，树 $T$ 的叶结点 $t$ 有 $N_{t}$ 个样本点，其中属于 $k$ 类的样本点有 $N_{tk}$ 个，$k=1,2,...,K$，$H_{t}(T)$ 为叶结点 $t$ 上的经验熵，$\alpha \geqslant 0$ 为参数，决策树学习的损失函数为：
$$C_{\alpha }\left ( T \right )=\sum_{t=1}^{\left | T \right |}N_{t}H_{t}\left ( T \right )+\alpha \left | T \right |$$
$$H_{t}\left ( T \right )=-\sum_{k=1}^{K}\frac{\left | N_{tk} \right |}{\left | N_{t} \right |}log\frac{\left | N_{tk} \right |}{\left | N_{t} \right |}$$  
记
$$C(T)=\sum_{t=1}^{\left | T \right |}N_{t}H_{t}\left ( T \right )$$  
有
$$C_{\alpha }\left ( T \right )=C(T)+\alpha \left | T \right |$$  
$C(T)$ 表示模型对训练数据的预测误差，即对训练数据的拟合程度，$\left | T \right |$ 表示模型的复杂度， 参数 $\alpha$ 控制二者之间的影响。决策树剪枝就是选择损失函数最小的模型。上式定义的损失函数的极小化等价于正则化的极大似然估计。  

**决策树剪枝算法**
>输入：生成算法产生的整个树 $T$，参数 $\alpha$；  
>输出：剪枝后的子树 $T_{a}$.
>1. 计算每个结点的经验熵。  
>2. 递归地从树的叶结点向上回缩  
>设一组叶结点回缩到其父节点之前与之后的整体树分别为 $T_{B}$ 与 $T_{A}$，如果  
>$$C_{\alpha }\left ( T_{A} \right ) \leqslant C_{\alpha }\left ( T_{B} \right )$$
>>则进行剪枝，将父结点变为新的叶结点  
>3. 返回2，直到不能继续进行为止，得到损失函数最小的子树 $T_{a}$. 
## CART算法（分类与回归树）