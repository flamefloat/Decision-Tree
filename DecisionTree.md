# 决策树
## 决策树模型
决策树是一种基本的分类回归方法，决策树由结点与有向边组成。结点分为內部结点与叶结点。內部结点表示一个特征或属性，叶结点表示一个类。决策树学习包括三个步骤：特征选择、决策树生成与决策树剪枝。

决策树等价于一个 ```if-then``` 规则的集合。决策树的根节点到叶结点的每一条路径构建一条规则；路径上的內部结点对应着规则的条件，叶结点对应规则的结论。决策树的规则的性质：互斥且完备，即每个实例都被一条且仅有一条路径覆盖。

叶结点中大多数实例属于某个类，则将该叶结点划分为该类。

决策树学习本质上是从训练数据中归纳出一组分类规则，与训练集不相矛盾矛盾的树可能有多个或者没有，应选择与训练数据矛盾较小的树同时具有较好的泛化能力。决策树学习也可以看作是由训练数据集估计条件概率模型。决策树的损失函数通常为正则化的极大似然函数。

从所有可能的决策树中选取最优的决策树是 NP 完全问题，所以实际中一般采用启发式的学习算法，求解次优决策树。决策树的学习算法通常是一个递归的选择最优特征，并根据该特征对训练数据进行分割，使得对各个子数据集有一个最好的分类过程。

选取最优特征最为根结点，按照这一特征将训练集分割成子集，使得各子集有一个当前条件下的最好分类。如果这些子集能够被基本分类正确，则构建叶结点，并将子集分到对应叶结点中；如果还有子集不能被基本分类正确，则对这些子集选择新的最优特征，继续对其进行分割，构建相应的结点。如此递归的进行下去，直至所有训练数据子集被基本分类正确或没有合适特征为止。最后每一个子集都被分到叶结点上，就生成了一颗决策树。

为避免发生过拟合现象，还需对决策树进行自下而上的**剪枝**，决策树的生成对应模型的局部选择（局部最优），决策树的剪枝对应模型的全局选择（全局最优）。
## 特征选择
特征选择的准则是信息增益或信息增益比。对于训练集 $D$ 和特征$A$，记：

$\left | D \right |$为样本容量，设有$K$个类$C_{k}$ , $k=1,2,...,K$, $\left | C_{k} \right |$表示属于类$C_{k}$的样本个数。$\sum_{k=1}^{K}\left | C_{k} \right |=\left | D \right |$, 设特征$A$有$n$个不同取值，根据特征$A$的取值能将$D$划分为$n$个子集$D_{1},D_{2},...,D_{n}$ ，$\sum_{i=1}^{n}\left | D_{i} \right |=\left | D \right |$， 记子集$D_{i}$中属于类$C_{k}$的集合为$D_{ik}$，$\sum_{k=1}^{K}\left | D_{ik} \right |=\left | D_{i}\right |$。信息增益计算如下：


$$H\left ( D \right )=-\sum_{k=1}^{K}\frac{\left | C_{k} \right |}{\left | D \right |}log_{2}\frac{\left | C_{k} \right |}{\left | D \right |}$$
$$H\left ( D|A \right )=\sum_{i=1}^{n}\frac{\left | D_{i} \right |}{\left | D \right |}H\left ( D_{i} \right )=-\sum_{i=1}^{n}\frac{\left | D_{i} \right |}{\left | D \right |}\sum_{k=1}^{K}\frac{\left | D_{ik} \right |}{\left | D_{i} \right |}log_{2}\frac{\left | D_{ik} \right |}{\left | D_{i} \right |}$$
$$g\left ( D,A \right )=H\left ( D \right )-H\left ( D|A \right )$$
信息增益实际上为类 $D$ 和特征 $A$ 之间的互信息。

条件熵：
$$H\left ( Y|X \right )=\sum_{i=1}^{n}p_{i}H\left ( Y|X=x_{i} \right )$$
$$p_{i}=P\left ( X=x_{i} \right )$$

以信息熵作为划分训练数据集的特征，存在偏向于选择取值较多的特征的问题，使用信息增益比可对该问题进行矫正。信息增益比：
$$g_{R}\left ( D,A \right )=\frac{g\left ( D,A \right )}{H_{A}\left ( D \right )}$$
$$H_{A}\left ( D \right )=-\sum_{i=1}^{n}\frac{\left | D_{i} \right |}{\left | D \right |}log_{2}\frac{\left | D_{i} \right |}{\left | D \right |}=H\left ( A \right )$$
## 决策树的生成
**ID3算法**
>输入：训练数据集$D$，特征集$A$，阈值 $\varepsilon$；
> 
>输出：决策树 $T$
>
>1. 若 $D$ 中所有实列输入同一类 $C_{k}$，则 $T$ 为单结点树，并将类 $C_{k}$ 作为该结点的类标记，返回$T$;  
>2. 若 $A=\varnothing$，则 $T$ 为单结点树，并将 $D$ 中实列数量最大的类 $C_{k}$ 作为该结点的类标记，返回$T$;  
>3. 否则，计算 $A$ 中各特征对 $D$ 的信息增益，选择信息增益最大的特征 $A_{g}$；  
>4. 如果 $A_{g}$ 的信息增益小于阈值 $\varepsilon$，则置 $T$ 为单结点树，将 $D$ 中实列数量最大的类 $C_{k}$ 作为该结点的类标记，返回$T$;  
>5. 否则，对 $A_{g}$ 的每一可能取值 $a_{i}$，依 $A_{g}=a_{i}$ 将 $D$ 分割为若干非空子集 $D_{i}$，将 $D_{i}$ 中实例数量最大的类最为标记，构建子节点，由结点及其子结点构成树 $T$，返回 $T$;  
>6. 对第 $i$ 个子结点，以 $D_{i}$ 为训练集，以 $A-\{A_{g}\}$ 为特征集，递归调用 1—5，得到子树 $T_{i}$，返回 $T_{i}$；

**ID3算法只有树的生成，所以该算法生成的树容易产生过拟合**  
**C4.5算法与ID3算法相似，不过在特征选择上将信息增益用信息增益比替代**  

## 决策树的剪枝
决策树生成过程中过多考虑提高对训练集的分类准确性，从而构建出过于复杂的决策树。可以通过简化决策树（剪枝）来解决过拟合的问题。  
决策树剪枝通过极小化决策树的整体的损失函数来实现。设树 $T$ 的叶结点个数为 $\left | T \right |$，树 $T$ 的叶结点 $t$ 有 $N_{t}$ 个样本点，其中属于 $k$ 类的样本点有 $N_{tk}$ 个，$k=1,2,...,K$，$H_{t}(T)$ 为叶结点 $t$ 上的经验熵，$\alpha \geqslant 0$ 为参数，决策树学习的损失函数为：
$$C_{\alpha }\left ( T \right )=\sum_{t=1}^{\left | T \right |}N_{t}H_{t}\left ( T \right )+\alpha \left | T \right |$$
$$H_{t}\left ( T \right )=-\sum_{k=1}^{K}\frac{\left | N_{tk} \right |}{\left | N_{t} \right |}log\frac{\left | N_{tk} \right |}{\left | N_{t} \right |}$$  
记
$$C(T)=\sum_{t=1}^{\left | T \right |}N_{t}H_{t}\left ( T \right )$$  
有
$$C_{\alpha }\left ( T \right )=C(T)+\alpha \left | T \right |$$  
$C(T)$ 表示模型对训练数据的预测误差，即对训练数据的拟合程度，$\left | T \right |$ 表示模型的复杂度， 参数 $\alpha$ 控制二者之间的影响。决策树剪枝就是选择损失函数最小的模型。上式定义的损失函数的极小化等价于正则化的极大似然估计。  

**决策树剪枝算法**
>输入：生成算法产生的整个树 $T$，参数 $\alpha$；  
>输出：剪枝后的子树 $T_{a}$.
>1. 计算每个结点的经验熵。  
>2. 递归地从树的叶结点向上回缩  
>设一组叶结点回缩到其父节点之前与之后的整体树分别为 $T_{B}$ 与 $T_{A}$，如果  
>$$C_{\alpha }\left ( T_{A} \right ) \leqslant C_{\alpha }\left ( T_{B} \right )$$
>>则进行剪枝，将父结点变为新的叶结点  
>3. 返回2，直到不能继续进行为止，得到损失函数最小的子树 $T_{a}$. 
## CART算法（分类与回归树）
CART算法是在给定输入随机变量 $X$ 条件下输出随机变量 $Y$ 的条件概率分布的学习方法。CART假设决策树是二叉树，内部结点特征取值为是和否，左分支为是，右分支为否。这样的决策树等价于递归的二分每个特征。  
决策树生成：基于训练数据生成决策树，生成的决策树要尽量大。
决策树剪枝：根据最小化损失函数对已生成的树剪枝，得到最优子树。   
对回归树用平方误差最小准则，对分类树用基尼指数最小化准则进行特征选择，生成二叉树。 

**最小二乘回归树生成算法**
>输入：训练数据集 $D$；  
>输出：回归树 $f(x)$.
>1. 选择最优切分变量 $j$ 与切分点 $s$，求解：
>$$\underset{j,s}{min}\left [  \underset{c_{1}}{min}\sum_{x_{i}\in R_{1}\left ( j,s \right )}\left ( y_{i}- c_{1}\right )^{2}+\underset{c_{2}}{min}\sum_{x_{i}\in R_{2}\left ( j,s \right )}\left ( y_{i}- c_{2}\right )^{2}\right ]$$
>>遍历变量 $j$ ，对固定的切分变量 $j$ 扫描切分点 $s$ ，选择使上式值最小的变量对 $(j,s)$.  
>2. 用选定的 $(j,s)$ 划分区域并决定相应的输出值：
>$$R_{1}\left ( j,s \right )=\left \{ x|x^{\left ( j \right ) }\leqslant s \right \},R_{2}\left ( j,s \right )=\left \{ x|x^{\left ( j \right ) }>  s \right \}$$
>$$\widehat{c_{m}}=\frac{1}{N_{m}}\sum_{x_{i}\in R_{m}\left ( j,s \right )}y_{i},x\in R_{m},m=1,2$$
>3. 继续对两个子区域调用步骤1、2，直至满足停止条件。
>4. 将输入空间划分为 $M$ 个区域 $R_{1},R_{2},...,R_{M}$，生成决策树：
>$$f\left ( x \right )=\sum_{m=1}^{M}\widehat{c_{m}}I\left ( x\in R_{m} \right )$$
 
 基尼指数  （近似代表分类误差率）  
 在分类问题中，假设有 $K$ 类，样本点属于类的概率为 $p_{k}$，则概率分布的基尼指数为：
 $$Gini\left ( p \right )=\sum_{k=1}^{K}p_{k}\left ( 1-p_{k} \right )=1-\sum_{k=1}^{K}p_{k}^{2}$$  
 对于给定的样本集合 $D$，其基尼指数为：
 $$Gini\left ( D \right )=1-\sum_{k=1}^{K}\left (\frac{\left | C_{k} \right |}{\left | D \right |}  \right )^{2}$$  
 样本集和 $D$ 根据特征 $A$ 是否取某一可能值 $a$ 被分割为 $D_{1}$ 和 $D_{2}$ 两个部分，即
 $$D_{1}=\left \{ \left ( x,y \right ) \in D|A\left ( x \right )=a\right \},\; \; D_{2}=D-D_{1}$$
 则在特征 $A$ 的条件下，集合 $D$ 的基尼指数定义为
 $$Gini\left ( D,A \right )=\frac{\left | D_{1} \right |}{\left | D \right |}Gini\left ( D_{1} \right )+\frac{\left | D_{2} \right |}{\left | D \right |}Gini\left ( D_{2} \right )$$

 **CART生成算法**
>输入：训练数据集 $D$，停止计算的条件；  
>输出：CART决策树.
>根据训练数据集，从根结点开始，递归地对每个结点进行如下操作，构建二叉决策树：  
>1. 设结点的训练数据集为 $D$，计算现有特征对该数据集的基尼指数。此时，对每个特征 $A$，对其可能的每个取值 $a$，根据样本点对 $A=a$ 测试为“是”或“否”，将 $D$ 分割为 $D_{1}$ 和 $D_{2}$ 两个部分，并计算 $A=a$ 时的基尼指数。  
>2. 将所有可能的特征 $A$ 以及它们所有可能的切分点 $a$ 中，选择基尼指数最小的特征及其对应的切分点作为最优特征与最优切分点。依最优特征与最优切分点，从现结点生成两个子结点，将训练数据集分配到两个子结点中去。  
>3. 对两个子结点递归调用1、2，直至满足条件。  
>4. 生成CART决策树。  
>算法停止条件：结点中样本小于预定阈值，或样本集中基尼指数小于预定阈值，或没有更多特征。

以 $t$ 为单结点树的损失函数：
>$$C_{a}(t)=C(t)+a$$  

以 $t$ 为根结点的子树 $T_{t}$ 的损失函数：
>$$C_{a}(T_{t})=C(T_{t})+a\left | T_{t} \right |$$

 **CART剪枝算法**
 >输入：CART算法生成的决策树 $T_{0}$；  
 >输出：最优决策树  $T_{a}$
 >1. 设 $k=0,T=T_{0}$.
 >2. 设 $a = +\infty$.
 >3. 自下而上对内部结点 $t$ 计算 $C(T_{t})$，$\left | T_{t} \right |$ 以及
 >$$g\left ( t \right )=\frac{C\left ( t \right )-C\left ( T_{t} \right )}{\left | T_{t} \right |-1}$$
 >$$a=min(a,g(t))$$
 >>$T_{t}$ 表示以 $t$ 为根结点的子树，$C(t)$ 是以 $t$ 为单结点树对训练数据的预测误差(可以为基尼系数)，$C(T_{t})$ 是对训练数据的预测误差，$\left | T_{t} \right |$ 是 $T_{t}$ 的叶结点个数。  
 >4. 对 $g(t)=a$ 的內部结点 $t$ 进行剪枝，并对叶结点 $t$ 以多数表决法决定其类，得到树 $T$。  
 >5. 令 $k=k+1,a_{k}=a,T_{k}=T$。  
 >6. 如果不是由根结点及两个叶结点构成的树，则返回步骤（3）；否则令 $T_{k}=T_{n}$。  
 >7. 采用交叉验证法在子树序列 $T_{0},T_{1},...,T_{n}$ 中选取最优子树 $T_{a}$。