# 决策树
## 决策树模型
决策树是一种基本的分类回归方法，决策树由结点与有向边组成。结点分为內部结点与叶结点。內部结点表示一个特征或属性，叶结点表示一个类。决策树学习包括三个步骤：特征选择、决策树生成与决策树剪枝。

决策树等价于一个 ```if-then``` 规则的集合。决策树的根节点到叶结点的每一条路径构建一条规则；路径上的內部结点对应着规则的条件，叶结点对应规则的结论。决策树的规则的性质：互斥且完备，即每个实例都被一条且仅有一条路径覆盖。

叶结点中大多数实例属于某个类，则将该叶结点划分为该类。

决策树学习本质上是从训练数据中归纳出一组分类规则，与训练集不相矛盾矛盾的树可能有多个或者没有，应选择与训练数据矛盾较小的树同时具有较好的泛化能力。决策树学习也可以看作是由训练数据集估计条件概率模型。决策树的损失函数通常为正则化的极大似然函数。

从所有可能的决策树中选取最优的决策树是 NP 完全问题，所以实际中一般采用启发式的学习算法，求解次优决策树。决策树的学习算法通常是一个递归的选择最优特征，并根据该特征对训练数据进行分割，使得对各个子数据集有一个最好的分类过程。

选取最优特征最为根结点，按照这一特征将训练集分割成子集，使得各子集有一个当前条件下的最好分类。如果这些子集能够被基本分类正确，则构建叶结点，并将子集分到对应叶结点中；如果还有子集不能被基本分类正确，则对这些子集选择新的最优特征，继续对其进行分割，构建相应的结点。如此递归的进行下去，直至所有训练数据子集被基本分类正确或没有合适特征为止。最后每一个子集都被分到叶结点上，就生成了一颗决策树。

为避免发生过拟合现象，还需对决策树进行自下而上的**剪枝**，决策树的生成对应模型的局部选择（局部最优），决策树的剪枝对应模型的全局选择（全局最优）。
## 特征选择
特征选择的准则是信息增益或信息增益比。对于训练集 $D$ 和特征$A$，记：

$\left | D \right |$为样本容量，设有$K$个类$C_{k}$ , $k=1,2,...,K$, $\left | C_{k} \right |$表示属于类$C_{k}$的样本个数。$\sum_{k=1}^{K}\left | C_{k} \right |=\left | D \right |$, 设特征$A$有$n$个不同取值，根据特征$A$的取值能将$D$划分为$n$个子集$D_{1},D_{2},...,D_{n}$ ，$\sum_{i=1}^{n}\left | D_{i} \right |=\left | D \right |$， 记子集$D_{i}$中属于类$C_{k}$的集合为$D_{ik}$，$\sum_{k=1}^{K}\left | D_{ik} \right |=\left | D_{i}\right |$。信息增益计算如下：


$$H\left ( D \right )=-\sum_{k=1}^{K}\frac{\left | C_{k} \right |}{\left | D \right |}log_{2}\frac{\left | C_{k} \right |}{\left | D \right |}$$
$$H\left ( D|A \right )=\sum_{i=1}^{n}\frac{\left | D_{i} \right |}{\left | D \right |}H\left ( D_{i} \right )=-\sum_{i=1}^{n}\frac{\left | D_{i} \right |}{\left | D \right |}\sum_{k=1}^{K}\frac{\left | D_{ik} \right |}{\left | D_{i} \right |}log_{2}\frac{\left | D_{ik} \right |}{\left | D_{i} \right |}$$
$$g\left ( D,A \right )=H\left ( D \right )-H\left ( D|A \right )$$
信息增益实际上为类 $D$ 和特征 $A$ 之间的互信息。

条件熵：
$$H\left ( Y|X \right )=\sum_{i=1}^{n}p_{i}H\left ( Y|X=x_{i} \right )$$
$$p_{i}=P\left ( X=x_{i} \right )$$

以信息熵作为划分训练数据集的特征，存在偏向于选择取值较多的特征的问题，使用信息增益比可对该问题进行矫正。信息增益比：
$$g_{R}\left ( D,A \right )=\frac{g\left ( D,A \right )}{H_{A}\left ( D \right )}$$
$$H_{A}\left ( D \right )=-\sum_{i=1}^{n}\frac{\left | D_{i} \right |}{\left | D \right |}log_{2}\frac{\left | D_{i} \right |}{\left | D \right |}=H\left ( A \right )$$
## 决策树的生成